{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ekplor Unigram-Bigram.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOPc/hEP/DclokoRzORZXM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muzakkialfarisi/MACHINE-LEARNING/blob/master/Ekplor_Unigram_Bigram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyrcfASltDh2"
      },
      "source": [
        "**Import kebutuhan library python**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_cW07gwvZV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc34e13-a326-4c85-d9eb-c3f60f6de823"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # untuk mengunduh package punkt, untuk tokenisasi.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/') # untuk dapat mengakses file / folder didalam drive\n",
        "from collections import Counter # untuk sorting dictionary\n",
        "import math # untuk dapat menggunakan operasi matematika"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJdyvy0bunh0"
      },
      "source": [
        "**Fungsi tokenisasi, sekaligus mengecilkan semua text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohlvXtVbwCIq"
      },
      "source": [
        "def nltk_lowercase(text): # menggunakan parameter dari gabungan variable artikel\n",
        "  return nltk.sent_tokenize(text.lower()) # tokenisasi menjadi beberapa kalimat dan lowercase semua text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stVyA9lWu1EP"
      },
      "source": [
        "**Fungsi tokenisasi menjadi beberapa kalimat, lalu menjadi beberapa token.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_61L9HWxTER"
      },
      "source": [
        "def tokenisasi(artikel): # menggunakan variable return dari fungsi nltk_lowercase(-)\n",
        "  gudang_kalimat = [] # array/list untuk menampung semua kalimat teks awal\n",
        "  for kalimat in artikel: # loop per kalimat, tambahkan tag <s> di awal kalimat dan </s> di akhir kalimat\n",
        "      mod_sentence = []\n",
        "      sent_tokens = nltk.word_tokenize(kalimat)\n",
        "      mod_sentence.append('<s>')\n",
        "      for token in sent_tokens:\n",
        "        mod_sentence.append(token) # kumpulan kata pada 1 kalimat\n",
        "      mod_sentence.append('</s>')\n",
        "      gudang_kalimat.append(mod_sentence) # kumpulan kalimat pada artikel\n",
        "  return gudang_kalimat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU_S5xP2vDK4"
      },
      "source": [
        "**Fungsi perhitngan frekuensi kemunculan unigram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFJl3TJDyJkY"
      },
      "source": [
        "def freq_unigram(gudang_kalimat): # menggunakna variable return dari fungsi tokenisasi(-)\n",
        "  freq_unigram = {} # dictionary untuk unigram\n",
        "  for kalimat in gudang_kalimat:\n",
        "    for kata in kalimat:\n",
        "      if kata in freq_unigram:\n",
        "          freq_unigram[kata] += 1 # kata sudah ada di dictionary, update frekuensinya\n",
        "      else:\n",
        "          freq_unigram[kata] = 1 # kata belum ada di dictionary \n",
        "  return freq_unigram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUMl7Z37yHer"
      },
      "source": [
        "**Fungsi perhitngan probabilitas dari unigram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9DInAdM2NDI"
      },
      "source": [
        "def prob_unigram(freq_unigram): # menggunakan variable return dari fungsi freq_unigram(-)\n",
        "  total_freq = sum(freq_unigram[item] for item in freq_unigram) # mengitung total value dari frequensi unigram\n",
        "  prob_unigram = {} # dictionary untuk probabilitas unigram\n",
        "  for token in freq_unigram:\n",
        "    prob_unigram[token] = freq_unigram[token]/total_freq # membagi nilai frequensi kata[token] dengan nilai total frequensi\n",
        "  return prob_unigram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRxM7PdMzKIY"
      },
      "source": [
        "**Fungsi mencari 10 frequensi tertinggi unigram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogqjo8zMyxTa"
      },
      "source": [
        "def top10_freq_unigram(unigram): # menggunkana variabe return dari fungsi freq_unigram(-)\n",
        "  top10_unigram = Counter(unigram) # Library python, sorting dictionary value secara descending\n",
        "  return top10_unigram.most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6im1K-W-3D7n"
      },
      "source": [
        "**Fungsi perhitungan frequensi kemunculan bigram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WTNIWD88wWh"
      },
      "source": [
        "def freq_bigram(gudang_kalimat): # menggunakan variable return dari fungsi tokenisasi(-)\n",
        "  freq_bigram = {} # dictionary untuk frequensi bigram\n",
        "  for kalimat in gudang_kalimat:\n",
        "    for i in range (1, len(kalimat)):\n",
        "      curr_bigram = (kalimat[i-1], kalimat[i]) # memasangkan kata sebekumnya dengan kata yang sekarang\n",
        "      if curr_bigram in freq_bigram:\n",
        "        freq_bigram[curr_bigram] += 1 # bigram sudah ada di dictionary, update frekuensinya\n",
        "      else:\n",
        "        freq_bigram[curr_bigram] = 1 # bigram belum ada di dictionary\n",
        "  return freq_bigram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQwlWM7OINUh"
      },
      "source": [
        "**Fungsi perhitngan probabilitas dari bigram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_ahDwVr-zwt"
      },
      "source": [
        "def prob_bigram(gudang_kalimat): # menggunakan variable return dari fungsi tokenisasi(-)\n",
        "  freq_uni = freq_unigram(gudang_kalimat) # memanggil fungsi freq_unigram(-) untuk mendapatkan nilai frequensi unigram\n",
        "  freq_bi = freq_bigram(gudang_kalimat) # memanggil fungsi freq_bigram(-) untuk mendapatkan nilai frequensi bigram\n",
        "  prob_bigram = {} # dictionary untuk probabilitas bigram\n",
        "  for sentence in gudang_kalimat:\n",
        "    for i in range (1, len(sentence)):\n",
        "      curr_bigram = (sentence[i-1], sentence[i]) # memasangkan kata sebekumnya dengan kata yang sekarang\n",
        "      if curr_bigram not in prob_bigram: # ketika pasangan bigram saat ini tdk dalam dictionary probabilitas bigram\n",
        "        prob_bigram[curr_bigram] = freq_bi[curr_bigram]/freq_uni[sentence[i-1]] # membagi frequensi bigram saat ini dengan kata ke-1 dari pasangan bigram\n",
        "  return prob_bigram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-y03Kh6Idaw"
      },
      "source": [
        "**Fungsi mencari 10 probabilitas tertinggi bigram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbcMFnfgAvoc"
      },
      "source": [
        "def top10_prob_bigram(bigram): # menggunkana variabe return dari fungsi prob_bigram(-)\n",
        "  top = Counter(bigram) # Library python, sorting dictionary value secara descending\n",
        "  return top.most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSKmg6W0jKMF"
      },
      "source": [
        "**Fungsi Smoothing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5fV4WSR7gfZ"
      },
      "source": [
        "def smoothing(uji, latih):\n",
        "  step_1 = nltk_lowercase(uji) # tokenisasi kalimat uji dan mengecilkan semua huruf\n",
        "  step_2 = tokenisasi(step_1) # memecah menjadi beberapa kalimat dan menambahkan tag <s> diawal dan tag </s> diakhir\n",
        "\n",
        "  freq_1 = freq_unigram(latih) # menyimpan frequensi unigram dari data latih\n",
        "  freq_2 = freq_bigram(latih) # menyimpan frequensi bigram dari data latih\n",
        "  temp_prob = []\n",
        "  for kalimat in step_2: # loop setiap kalimat di data uji\n",
        "    probabilitas = 1\n",
        "    temp = 1\n",
        "    for i in range (1, len(kalimat)): # loop kata sepanjang kalimat didalam data uji\n",
        "      curr_bigram = (kalimat[i-1], kalimat[i]) # penggabungan kata menjadi bigram dari data uji\n",
        "      if curr_bigram in freq_2: # ketika bigram ada di dictionary\n",
        "        pembilang = freq_2[curr_bigram] + 1\n",
        "        penyebut = freq_1[kalimat[i-1]]+len(freq_1)\n",
        "      else:\n",
        "        if kalimat[i-1] in freq_1: # ketika bigram tdk ada di dic dan unigram ada di dic\n",
        "          pembilang = 1\n",
        "          penyebut = freq_1[kalimat[i-1]]+len(freq_1)\n",
        "        else: # ketika bigram tdk ada di dic dan unigram tidak ada di dic\n",
        "          pembilang = 1\n",
        "          penyebut = 1+len(freq_1)\n",
        "      probabilitas = pembilang/penyebut\n",
        "      temp = temp*probabilitas # probabilitas per kalimat\n",
        "    temp_prob.append(temp) # masukan array\n",
        "  return temp_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opLqCQymxw8o"
      },
      "source": [
        "**Fungsi Perplexity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7tQdzkqxvtZ"
      },
      "source": [
        "def perplex(uji, prob):\n",
        "  step_1 = nltk_lowercase(uji) # tokenisasi kalimat uji dan mengecilkan semua huruf\n",
        "  step_2 = tokenisasi(step_1) # memecah menjadi beberapa kalimat dan menambahkan tag <s> diawal dan tag </s> diakhir\n",
        "  temp_len, temp_per = [], []\n",
        "  for kalimat in step_2:\n",
        "    temp_len.append(len(kalimat)) # menyimpan panjang tiap kalimat dalam array\n",
        "  for i in range(len(step_2)):\n",
        "    temp_per.append(prob[i]**-(1/temp_len[i])) # formula perplexity = prob kalimat uji ^ - (1/panjang kalimatnya)\n",
        "  return temp_per"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8HQtvIbTH-b"
      },
      "source": [
        "**MAIN PROGRAM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "art8CDKpTd_y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "f56e49ae-8907-4c63-c3e2-bdf40e9305b7"
      },
      "source": [
        "# membaca file dari drive\n",
        "a1 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article1.txt').read()\n",
        "a2 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article2.txt').read()\n",
        "a3 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article3.txt').read()\n",
        "a4 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article4.txt').read()\n",
        "a5 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article5.txt').read()\n",
        "a6 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article6.txt').read()\n",
        "a7 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article7.txt').read()\n",
        "a8 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article8.txt').read()\n",
        "a9 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article9.txt').read()\n",
        "a10 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article10.txt').read()\n",
        "a11 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article11.txt').read()\n",
        "a12 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article12.txt').read()\n",
        "a13 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article13.txt').read()\n",
        "a14 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article14.txt').read()\n",
        "a15 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article15.txt').read()\n",
        "a16 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article16.txt').read()\n",
        "a17 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article17.txt').read()\n",
        "a18 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article18.txt').read()\n",
        "a19 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article19.txt').read()\n",
        "a20 = open('/content/drive/My Drive/Colab Notebooks/DataTrain/article20.txt').read()\n",
        "\n",
        "# menggabungkan artikel\n",
        "article = a1 + a2 + a3 + a4 + a5 + a6 + a7 + a8 + a9 + a10 + a11 + a12 + a13 + a14 + a15 + a16 + a17 + a18 + a19 + a20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-56358a994c79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# membaca file dari drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Colab Notebooks/DataTrain/article1.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Colab Notebooks/DataTrain/article2.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Colab Notebooks/DataTrain/article3.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ma4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Colab Notebooks/DataTrain/article4.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/DataTrain/article1.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8-Bqtqw3CE7"
      },
      "source": [
        "latih = tokenisasi(nltk_lowercase(article)) # fungsi tokenisasi, hasilnya diubah tokenisasi menjadi beberapa kalimat lalau menjadi token\n",
        "file_uji = open('/content/drive/My Drive/Colab Notebooks/DataTest/uji.txt').read() # membaca file data uji\n",
        "\n",
        "prob_uji = smoothing(file_uji, latih) # fungsi smoothing\n",
        "prex_uji = perplex(file_uji,prob_uji) #fungsi perplexity\n",
        "\n",
        "temp1 = 1\n",
        "temp2 = 1\n",
        "\n",
        "print('-- - TOPIK SERUPA - --')\n",
        "for i in range(3):\n",
        "  print('Probability Uji ke-',i+1,'=',prob_uji[i]) # print index ke 0 - 2\n",
        "  temp1 = prob_uji[i]*temp1\n",
        "print('Probabilitas Topik serupa :', temp1)\n",
        "print('\\n-- - TOPIK BEDA - --')\n",
        "for i in range(3,6): \n",
        "  print('Probability Uji ke-',i-2,'=',prob_uji[i]) # print index ke 3 - 5\n",
        "  temp2 = prob_uji[i]*temp2\n",
        "print('Probabilitas Topik berbeda :', temp2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3pqB5oi7g8a"
      },
      "source": [
        "print('-- - TOPIK SERUPA - --')\n",
        "for i in range(3):\n",
        "  print('Perplexity Uji ke-',i+1,'=',prex_uji[i]) # print index ke 0 - 2\n",
        "\n",
        "print('\\n-- - TOPIK BEDA - --')\n",
        "for i in range(3,6): \n",
        "  print('Perplexity Uji ke-',i-2,'=',prex_uji[i]) # print index ke 3 - 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTqQYU3ILdyG"
      },
      "source": [
        "**UNIGRAM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2Ln1RA1MMXD"
      },
      "source": [
        "10 frequensi tertinggi unigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2mBALDOMNod"
      },
      "source": [
        "step1_unigram = freq_unigram(step2) # memanggil fungsi unutk menentukan probabilitas unigram dari data latih\n",
        "top10_unigram = top10_freq_unigram(step1_unigram) # memanggil fungsi untuk sorting descending frequensi unigram\n",
        "print('10 frequensi tertinggi unigram\\n')\n",
        "for i in range(10):\n",
        "  print(top10_unigram[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61E9niW3L1Aa"
      },
      "source": [
        "**BIGRAM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miRt-wEk-fVu"
      },
      "source": [
        "step2_bigram = prob_bigram(step2) # memanggil fungsi unutk menentukan probabilitas bigram dari data latih\n",
        "top10_bigram = top10_prob_bigram(step2_bigram) # memanggil fungsi untuk sorting descending probabilitas bigram\n",
        "print('10 probabilitas tertinggi bigram\\n')\n",
        "for i in range(10):\n",
        "  print(top10_bigram[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}